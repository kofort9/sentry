name: Test Sentries

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    types: [ opened, synchronize, reopened ]
    branches: [ main, master, develop ]

concurrency:
  group: sentry-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Always run on GitHub-hosted runners (fast, reliable)
  test-basic:
    runs-on: ubuntu-latest
    # Continue even if tests fail - sentries need to run to fix them
    continue-on-error: true
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install pytest pytest-cov pytest-mock
      
      - name: Basic import test
        run: |
          python -c "import sentries; print('Sentries imported successfully')"
      
      - name: Run unit tests
        id: unit-tests
        run: |
          # Run pytest and capture output to file, but don't fail the workflow
          pytest sentries/ --cov=sentries --cov-report=xml --tb=short -q > pytest-output.txt 2>&1 || true
          
          # Capture pytest exit code and output
          pytest_exit_code=$?
          echo "pytest_exit_code=$pytest_exit_code" >> $GITHUB_OUTPUT
          
          if [ $pytest_exit_code -eq 0 ]; then
            echo "test_status=passing" >> $GITHUB_OUTPUT
            echo "✅ All tests are passing"
          else
            echo "test_status=failing" >> $GITHUB_OUTPUT
            echo "❌ Some tests are failing - TestSentry will attempt to fix them"
            echo "📄 Test failure output saved to pytest-output.txt"
          fi
      
      - name: Test CLI installation
        run: |
          # Test that the core CLI commands are properly installed
          timeout 5s testsentry --help || echo "testsentry command available"
          timeout 5s docsentry --help || echo "docsentry command available"
      
      - name: Test import functionality
        run: |
          python -c "
          from sentries import banner, chat, prompts, diff_utils, git_utils, runner_common
          from sentries import testsentry, docsentry
          print('Core modules imported successfully')
          "
      

      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: false
      
      - name: Upload test results
        if: steps.unit-tests.outputs.test_status == 'failing'
        uses: actions/upload-artifact@v4
        with:
          name: test-failures-${{ matrix.python-version }}
          path: |
            pytest-output.txt
          retention-days: 1

  # Run on self-hosted runner only when LLM operations are needed
  test-sentry-llm:
    runs-on: self-hosted
    needs: test-basic
    if: github.event_name == 'pull_request' && github.event.pull_request.draft == false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      
      - name: Download test failure artifacts
        if: always()
        uses: actions/download-artifact@v4
        with:
          pattern: test-failures-*
          merge-multiple: true
      
      - name: Check Ollama Status
        id: ollama-check
        run: |
          if curl -sSf http://127.0.0.1:11434/api/tags >/dev/null; then
            echo "status=ready" >> $GITHUB_OUTPUT
            echo "✅ Ollama is running and ready"
          else
            echo "status=not-ready" >> $GITHUB_OUTPUT
            echo "❌ Ollama is not running"
            echo "Please start Ollama and rerun this workflow"
            exit 1
          fi
      
      - name: Check test status from artifacts
        id: pytest-check
        run: |
          echo "🧪 Checking test status from GitHub-hosted runner results..."
          
          # Check if we have test failure artifacts
          if [ -f "pytest-output.txt" ]; then
            echo "📄 Found test failure output from GitHub-hosted runners"
            echo "test_status=failing" >> $GITHUB_OUTPUT
            echo "❌ Tests are failing - TestSentry will attempt to fix them"
            echo "📋 Test failure details:"
            cat pytest-output.txt
          else
            echo "✅ No test failure artifacts found - tests are likely passing"
            echo "test_status=passing" >> $GITHUB_OUTPUT
          fi
      
      - name: Check if LLM jobs should run
        id: check-llm
        run: |
          # Always run LLM jobs on PRs to allow sentries to fix issues
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "reason=PR event - sentries should run to check for issues" >> $GITHUB_OUTPUT
            echo "✅ LLM jobs should run - PR event detected"
          else
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "reason=Not a PR event" >> $GITHUB_OUTPUT
            echo "✅ LLM jobs should run - not a PR event"
          fi
      
      - name: Install Sentries
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        run: |
          pip install -e .
      
      - name: Run TestSentry with LLM
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        env:
          LLM_BASE: http://127.0.0.1:11434
          MODEL_PLAN: ${{ vars.MODEL_PLAN || 'llama3.1:8b-instruct-q4_K_M' }}
          MODEL_PATCH: ${{ vars.MODEL_PATCH || 'deepseek-coder:6.7b-instruct-q5_K_M' }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REF: ${{ github.ref }}
        run: |
          echo "🚀 Running TestSentry to fix failing tests..."
          echo "Test status from pytest: ${{ steps.pytest-check.outputs.test_status }}"
          
          # Run TestSentry - it will automatically detect and fix test failures
          testsentry
      
      - name: Run DocSentry with LLM
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        env:
          LLM_BASE: http://127.0.0.1:11434
          MODEL_PLAN: ${{ vars.MODEL_PLAN || 'llama3.1:8b-instruct-q4_K_M' }}
          MODEL_PATCH: ${{ vars.MODEL_PATCH || 'deepseek-coder:6.7b-instruct-q5_K_M' }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REF: ${{ github.ref }}
          GITHUB_EVENT_PATH: ${{ github.event_path }}
        run: |
          docsentry
      
      - name: LLM Jobs Summary
        if: always()
        run: |
          if [ "${{ steps.check-llm.outputs.should-run }}" = "true" ]; then
            if [ "${{ steps.ollama-check.outputs.status }}" = "ready" ]; then
              echo "🎉 LLM jobs completed successfully!"
              echo "📋 Reason: ${{ steps.check-llm.outputs.reason }}"
              echo "🧪 Test status: ${{ steps.pytest-check.outputs.test_status }}"
              echo "🔗 Check the PR for any improvements created by sentries"
              
              if [ "${{ steps.pytest-check.outputs.test_status }}" = "failing" ]; then
                echo "💡 TestSentry attempted to fix failing tests - check for new PRs"
              fi
            else
              echo "❌ LLM jobs failed - Ollama not available"
            fi
          else
            echo "⏭️  LLM jobs skipped"
            echo "📋 Reason: ${{ steps.check-llm.outputs.reason }}"
            echo "💡 This is normal - LLM jobs only run on meaningful code changes"
          fi

  integration-test:
    runs-on: ubuntu-latest
    needs: test-basic
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      
      - name: Install dependencies
        run: |
          pip install -e .
          pip install pytest pytest-mock
      
      - name: Create test repository structure
        run: |
          # Create a mock repository structure to test sentries
          mkdir -p test-repo/tests test-repo/docs
          cd test-repo
          
          # Create a failing test
          cat > tests/test_example.py << 'EOF'
          def test_failing_function():
              assert 1 == 2  # This will fail
          
          def test_passing_function():
              assert 1 == 1  # This will pass
          EOF
          
          # Create a simple Python file
          cat > example.py << 'EOF'
          def example_function():
              return "Hello, World!"
          EOF
          
          # Initialize git
          git init
          git config user.name "Test User"
          git config user.email "test@example.com"
          git add .
          git commit -m "Initial commit with failing test"
          
          # Create a branch for testing
          git checkout -b test-branch
          
          # Modify a file to trigger doc updates
          echo "# Updated Documentation" > README.md
          git add README.md
          git commit -m "Update documentation"
          
          echo "Test repository created successfully"
      
      - name: Test sentries in mock environment
        run: |
          cd test-repo
          
          # Set environment variables for testing
          export GITHUB_REPOSITORY="test-org/test-repo"
          export GITHUB_REF="refs/heads/test-branch"
          
          # Test that sentries can be imported and run (without actual LLM calls)
          python -c "
          import sys
          sys.path.insert(0, '..')
          from sentries import testsentry, docsentry
          print('Sentries imported successfully in test environment')
          "
      
      - name: Cleanup test repository
        run: |
          rm -rf test-repo

  lint:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      
      - name: Install linting tools
        run: |
          pip install flake8 black isort mypy
      
      - name: Run flake8
        run: |
          flake8 sentries/ --max-line-length=100 --extend-ignore=E203,W503
      
      - name: Run black check
        run: |
          black --check --diff sentries/
      
      - name: Run isort check
        run: |
          isort --check-only --diff sentries/
      
      - name: Run mypy
        run: |
          mypy sentries/ --ignore-missing-imports --no-strict-optional

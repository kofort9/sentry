name: Test Sentries

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    types: [ opened, synchronize, reopened ]
    branches: [ main, master, develop ]
  workflow_dispatch:
    inputs:
      run_llm_tests:
        description: 'Run LLM-based TestSentry tests (requires self-hosted runner)'
        required: false
        default: false
        type: boolean

concurrency:
  group: sentry-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Always run on GitHub-hosted runners (fast, reliable)
  test-basic:
    runs-on: ubuntu-latest
    # Continue even if tests fail - sentries need to run to fix them
    continue-on-error: true
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install pytest pytest-cov pytest-mock
      
      - name: Basic import test
        run: |
          python -c "import sentries; print('Sentries imported successfully')"
      
      - name: Run unit tests
        id: unit-tests
        run: |
          # Run pytest and capture output to file, but don't fail the workflow
          pytest sentries/ --cov=sentries --cov-report=xml --tb=short -q > pytest-output.txt 2>&1 || true
          
          # Capture pytest exit code and output
          pytest_exit_code=$?
          echo "pytest_exit_code=$pytest_exit_code" >> $GITHUB_OUTPUT
          
          if [ $pytest_exit_code -eq 0 ]; then
            echo "test_status=passing" >> $GITHUB_OUTPUT
            echo "âœ… All tests are passing"
          else
            echo "test_status=failing" >> $GITHUB_OUTPUT
            echo "âŒ Some tests are failing - TestSentry will attempt to fix them"
            echo "ðŸ“„ Test failure output saved to pytest-output.txt"
          fi
      
      - name: Test CLI installation
        run: |
          # Test that the core CLI commands are properly installed
          timeout 5s testsentry --help || echo "testsentry command available"
          timeout 5s docsentry --help || echo "docsentry command available"
      
      - name: Test import functionality
        run: |
          python -c "
          from sentries import banner, chat, prompts, diff_utils, git_utils, runner_common
          from sentries import testsentry, docsentry
          print('Core modules imported successfully')
          "
      

      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: false
      
      - name: Upload test results
        if: steps.unit-tests.outputs.test_status == 'failing'
        uses: actions/upload-artifact@v4
        with:
          name: test-failures-${{ matrix.python-version }}
          path: |
            pytest-output.txt
          retention-days: 1

  # Run on self-hosted runner only when LLM operations are needed
  test-sentry-llm:
    runs-on: self-hosted
    needs: [test-basic, lint]
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.run_llm_tests == 'true') ||
      (github.event_name == 'pull_request' && 
       github.event.pull_request.draft == false && 
       (contains(github.event.pull_request.labels.*.name, 'test-llm') || 
        contains(github.event.pull_request.title, '[test-llm]') ||
        contains(github.event.pull_request.body, '[test-llm]')))
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      
      - name: Explain LLM test execution
        run: |
          echo "ðŸ¤– TestSentry LLM tests are running because:"
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "  - Manual trigger with run_llm_tests=true"
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "  - PR has 'test-llm' label, title, or body"
            echo "  - PR is not a draft"
          fi
          echo ""
          echo "ðŸ’¡ To skip LLM tests in the future:"
          echo "  - Remove 'test-llm' from PR title/body"
          echo "  - Remove 'test-llm' label from PR"
          echo "  - Or run workflow manually with run_llm_tests=false"
      
      - name: Download test failure artifacts
        if: always()
        uses: actions/download-artifact@v4
        with:
          pattern: test-failures-*
          merge-multiple: true
      
      - name: Check Ollama Status
        id: ollama-check
        run: |
          if curl -sSf http://127.0.0.1:11434/api/tags >/dev/null; then
            echo "status=ready" >> $GITHUB_OUTPUT
            echo "âœ… Ollama is running and ready"
          else
            echo "status=not-ready" >> $GITHUB_OUTPUT
            echo "âŒ Ollama is not running"
            echo "Please start Ollama and rerun this workflow"
            exit 1
          fi
      
      - name: Check test status from artifacts
        id: pytest-check
        run: |
          echo "ðŸ§ª Checking test status from GitHub-hosted runner results..."
          
          # Check if we have test failure artifacts
          if [ -f "pytest-output.txt" ]; then
            echo "ðŸ“„ Found test failure output from GitHub-hosted runners"
            echo "test_status=failing" >> $GITHUB_OUTPUT
            echo "âŒ Tests are failing - TestSentry will attempt to fix them"
            echo "ðŸ“‹ Test failure details:"
            cat pytest-output.txt
          else
            echo "âœ… No test failure artifacts found - tests are likely passing"
            echo "test_status=passing" >> $GITHUB_OUTPUT
          fi
      
      - name: Check if LLM jobs should run
        id: check-llm
        run: |
          # Run LLM jobs only after lint checks pass
          echo "âœ… Lint checks passed - proceeding with LLM operations"
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "reason=PR event and lint checks passed" >> $GITHUB_OUTPUT
            echo "âœ… LLM jobs should run - PR event detected and code quality verified"
          else
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "reason=Lint checks passed" >> $GITHUB_OUTPUT
            echo "âœ… LLM jobs should run - code quality verified"
          fi
      
      - name: Install Sentries
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        run: |
          pip install -e .
          pip install -e .[viz]
      
      - name: Setup Observability
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        run: |
          echo "ðŸ“Š Setting up observability metrics..."
          make setup
          make sample-data
          make benchmarks
          echo "âœ… Observability setup complete"
      
      - name: Run TestSentry with LLM
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        env:
          LLM_BASE: http://127.0.0.1:11434
          MODEL_PLAN: ${{ vars.MODEL_PLAN || 'llama3.1:8b-instruct-q4_K_M' }}
          MODEL_PATCH: ${{ vars.MODEL_PATCH || 'deepseek-coder:6.7b-instruct-q5_K_M' }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REF: ${{ github.ref }}
        run: |
          echo "ðŸš€ Running TestSentry to fix failing tests..."
          echo "Test status from pytest: ${{ steps.pytest-check.outputs.test_status }}"
          
          # Run TestSentry - it will automatically detect and fix test failures
          testsentry
      
      - name: Run DocSentry with LLM
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        env:
          LLM_BASE: http://127.0.0.1:11434
          MODEL_PLAN: ${{ vars.MODEL_PLAN || 'llama3.1:8b-instruct-q4_K_M' }}
          MODEL_PATCH: ${{ vars.MODEL_PATCH || 'deepseek-coder:6.7b-instruct-q5_K_M' }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REF: ${{ github.ref }}
          GITHUB_EVENT_PATH: ${{ github.event_path }}
        run: |
          docsentry
      
      - name: Generate Observability Reports
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        run: |
          echo "ðŸ“ˆ Generating observability reports..."
          make reports
          
          # Check if reports were generated
          if [ -f "reports/drift_psi_js.png" ] && [ -f "reports/scrubber_leakage.png" ] && [ -f "reports/drift_top_tokens.csv" ]; then
            echo "âœ… Observability reports generated successfully"
            ls -la reports/
          else
            echo "âš ï¸ Some reports may be missing"
            ls -la reports/ || echo "Reports directory not found"
          fi
      
      - name: Check Report Determinism
        if: steps.ollama-check.outputs.status == 'ready' && steps.check-llm.outputs.should-run == 'true'
        run: |
          echo "ðŸ” Checking report determinism..."
          make check-determinism
          echo "âœ… Reports are deterministic"
      
      - name: LLM Jobs Summary
        if: always()
        run: |
          if [ "${{ steps.check-llm.outputs.should-run }}" = "true" ]; then
            if [ "${{ steps.ollama-check.outputs.status }}" = "ready" ]; then
              echo "ðŸŽ‰ LLM jobs completed successfully!"
              echo "ðŸ“‹ Reason: ${{ steps.check-llm.outputs.reason }}"
              echo "ðŸ§ª Test status: ${{ steps.pytest-check.outputs.test_status }}"
              echo "ðŸ“Š Observability metrics collected and reports generated"
              echo "ðŸ”— Check the PR for any improvements created by sentries"
              
              # Add observability summary to GitHub step summary
              echo "## ðŸ“Š Observability Summary" >> $GITHUB_STEP_SUMMARY
              echo "- **PSI Last Week**: Check drift_psi_js.png for trend" >> $GITHUB_STEP_SUMMARY
              echo "- **PII Leakage Rate**: Check scrubber_leakage.png for performance" >> $GITHUB_STEP_SUMMARY
              echo "- **Top Tokens**: See drift_top_tokens.csv for token analysis" >> $GITHUB_STEP_SUMMARY
              echo "- **Reports Generated**: $(ls reports/*.png reports/*.csv 2>/dev/null | wc -l) files" >> $GITHUB_STEP_SUMMARY
              
              if [ "${{ steps.pytest-check.outputs.test_status }}" = "failing" ]; then
                echo "ðŸ’¡ TestSentry attempted to fix failing tests - check for new PRs"
              fi
            else
              echo "âŒ LLM jobs failed - Ollama not available"
            fi
          else
            echo "â­ï¸  LLM jobs skipped"
            echo "ðŸ“‹ Reason: ${{ steps.check-llm.outputs.reason }}"
            echo "ðŸ’¡ This is normal - LLM jobs only run on meaningful code changes"
          fi

  # Job that runs when LLM tests are skipped to provide clear feedback
  skip-llm-tests:
    runs-on: ubuntu-latest
    needs: [test-basic, lint]
    if: |
      github.event_name == 'pull_request' && 
      github.event.pull_request.draft == false &&
      !(contains(github.event.pull_request.labels.*.name, 'test-llm') || 
        contains(github.event.pull_request.title, '[test-llm]') ||
        contains(github.event.pull_request.body, '[test-llm]'))
    
    steps:
      - name: Explain LLM test skip
        run: |
          echo "â­ï¸  TestSentry LLM tests are being skipped"
          echo ""
          echo "ðŸ“‹ Current PR conditions:"
          echo "  - PR is not a draft: âœ…"
          echo "  - Has 'test-llm' label: âŒ"
          echo "  - Title contains '[test-llm]': âŒ"
          echo "  - Body contains '[test-llm]': âŒ"
          echo ""
          echo "ðŸ’¡ To run LLM tests, add one of the following:"
          echo "  - Add 'test-llm' label to this PR"
          echo "  - Add '[test-llm]' to PR title or body"
          echo "  - Or run workflow manually with run_llm_tests=true"
          echo ""
          echo "âœ… Basic tests and linting still ran successfully!"

  integration-test:
    runs-on: ubuntu-latest
    needs: [test-basic, lint]
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      
      - name: Install dependencies
        run: |
          pip install -e .
          pip install pytest pytest-mock
      
      - name: Create test repository structure
        run: |
          # Create a mock repository structure to test sentries
          mkdir -p test-repo/tests test-repo/docs
          cd test-repo
          
          # Create a failing test
          cat > tests/test_example.py << 'EOF'
          def test_failing_function():
              assert 1 == 2  # This will fail
          
          def test_passing_function():
              assert 1 == 1  # This will pass
          EOF
          
          # Create a simple Python file
          cat > example.py << 'EOF'
          def example_function():
              return "Hello, World!"
          EOF
          
          # Initialize git
          git init
          git config user.name "Test User"
          git config user.email "test@example.com"
          git add .
          git commit -m "Initial commit with failing test"
          
          # Create a branch for testing
          git checkout -b test-branch
          
          # Modify a file to trigger doc updates
          echo "# Updated Documentation" > README.md
          git add README.md
          git commit -m "Update documentation"
          
          echo "Test repository created successfully"
      
      - name: Test sentries in mock environment
        run: |
          cd test-repo
          
          # Set environment variables for testing
          export GITHUB_REPOSITORY="test-org/test-repo"
          export GITHUB_REF="refs/heads/test-branch"
          
          # Test that sentries can be imported and run (without actual LLM calls)
          python -c "
          import sys
          sys.path.insert(0, '..')
          from sentries import testsentry, docsentry
          print('Sentries imported successfully in test environment')
          "
      
      - name: Cleanup test repository
        run: |
          rm -rf test-repo

  lint:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      
      - name: Install linting tools
        run: |
          pip install flake8 black isort mypy
      
      - name: Run flake8
        run: |
          flake8 sentries/ --max-line-length=100 --extend-ignore=E203,W503
      
      - name: Run black check
        run: |
          black --check --diff sentries/
      
      - name: Run isort check
        run: |
          isort --check-only --diff sentries/
      
      - name: Run mypy
        run: |
          mypy sentries/ --ignore-missing-imports --no-strict-optional || echo "mypy check failed but continuing"
        continue-on-error: true
